""" 

Serves as the stage 3 generator for OMGM. 

"""

import torch
from .retriever import WikipediaKnowledgeBaseEntry
from transformers import (
    AutoTokenizer,
    AutoModel,
    AutoModelForCausalLM,
    AutoModelForSequenceClassification,
    LlavaForConditionalGeneration,
    AutoProcessor,
    AutoModelForVision2Seq,
    Qwen3VLForConditionalGeneration

)

import numpy as np
import torchvision.transforms as T
from torchvision.transforms.functional import InterpolationMode
from PIL import Image



VLM_INFOSEEK_INSTRUCTION = ("Answer the encyclopedic question about the given image." 
                    +"Don't mention the visual content of image in your output. "
                    +"Directly output the answer of the question according to the Context.\n"
                    +"If you need to answer questions about numbers or time, please output the corresponding numerical format directly.\n"
                    +"If the context does not contain the information required to answer the question, you should answer the question using internal model knowledge.\n "
                    +"There is an example:\n- Context: # Wiki Article: Dolomites\n## Section Title: Dolomites\nThe Dolomites, also known as the Dolomite Mountains, Dolomite Alps or Dolomitic Alps, are a mountain range located in northeastern Italy. The Dolomites are located in the regions of Veneto, Trentino-Alto Adige/Südtirol and Friuli Venezia Giulia, covering an area shared between the provinces of Belluno, Vicenza, Verona, Trentino, South Tyrol, Udine and Pordenone.\n"
                    +"- Question: Which city or region does this mountain locate in?\n"
                    +"Just answer the questions , no explanations needed. Short answer is: Province of Belluno\n\n")

VLM_INFOSEEK_VANILLA_INSTRUCTION = ("Answer the encyclopedic question about the given image." 
                    +"Don't mention the visual content of image in your output. "
                    +"Directly output the answer of the question.\n"
                    +"If you need to answer questions about numbers or time, please output the corresponding numerical format directly.\n")

VLM_EVQA_INSTRUCTION = ("Answer the encyclopedic question about the given image." 
                    +"Don't mention the visual content of image in your output. "
                    +"Directly output the answer of the question according to the Context."
                    +"If the context does not contain the information required to answer the question, you should answer the question using internal model knowledge.\n ")


VLM_EVQA_VANILLA_INSTRUCTION = ("Answer the encyclopedic question about the given image." 
                    +"Don't mention the visual content of image in your output. ")


IMAGENET_MEAN = (0.485, 0.456, 0.406)
IMAGENET_STD = (0.229, 0.224, 0.225)

def build_transform(input_size):
    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD
    transform = T.Compose([
        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),
        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),
        T.ToTensor(),
        T.Normalize(mean=MEAN, std=STD)
    ])
    return transform

def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):
    best_ratio_diff = float('inf')
    best_ratio = (1, 1)
    area = width * height
    for ratio in target_ratios:
        target_aspect_ratio = ratio[0] / ratio[1]
        ratio_diff = abs(aspect_ratio - target_aspect_ratio)
        if ratio_diff < best_ratio_diff:
            best_ratio_diff = ratio_diff
            best_ratio = ratio
        elif ratio_diff == best_ratio_diff:
            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:
                best_ratio = ratio
    return best_ratio

def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):
    orig_width, orig_height = image.size
    aspect_ratio = orig_width / orig_height

    # calculate the existing image aspect ratio
    target_ratios = set(
        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if
        i * j <= max_num and i * j >= min_num)
    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])

    # find the closest aspect ratio to the target
    target_aspect_ratio = find_closest_aspect_ratio(
        aspect_ratio, target_ratios, orig_width, orig_height, image_size)

    # calculate the target width and height
    target_width = image_size * target_aspect_ratio[0]
    target_height = image_size * target_aspect_ratio[1]
    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]

    # resize the image
    resized_img = image.resize((target_width, target_height))
    processed_images = []
    for i in range(blocks):
        box = (
            (i % (target_width // image_size)) * image_size,
            (i // (target_width // image_size)) * image_size,
            ((i % (target_width // image_size)) + 1) * image_size,
            ((i // (target_width // image_size)) + 1) * image_size
        )
        # split the image
        split_img = resized_img.crop(box)
        processed_images.append(split_img)
    assert len(processed_images) == blocks
    if use_thumbnail and len(processed_images) != 1:
        thumbnail_img = image.resize((image_size, image_size))
        processed_images.append(thumbnail_img)
    return processed_images

def load_image(image_file, input_size=448, max_num=12):
    image = Image.open(image_file).convert('RGB')
    transform = build_transform(input_size=input_size)
    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)
    pixel_values = [transform(image) for image in images]
    pixel_values = torch.stack(pixel_values)
    return pixel_values


def disable_torch_init():
    """
    Disable the redundant torch default initialization to accelerate model creation.
    """
    import torch

    setattr(torch.nn.Linear, "reset_parameters", lambda self: None)
    setattr(torch.nn.LayerNorm, "reset_parameters", lambda self: None)


def reconstruct_wiki_article(knowledge_entry: WikipediaKnowledgeBaseEntry):
    """Reconstruct the wiki article from the knowledge entry class."""
    title = knowledge_entry.title
    article = "# Wiki Article: " + title + "\n"
    for it, section_title in enumerate(knowledge_entry.section_titles):
        if (
            "external link" in section_title.lower()
            or "reference" in section_title.lower()
        ):
            continue
        article += (
            "\n## Section Title: "
            + section_title
            + "\n"
            + knowledge_entry.section_texts[it]
        )

    return article


def reconstruct_wiki_sections(knowledge_entry, section_index=-1):
    """Reconstruct the wiki sections from the knowledge entry class."""
    title = knowledge_entry.title
    sections = []
    for it, section_title in enumerate(knowledge_entry.section_titles):
        if it == int(section_index):
            evidence_section = (
                "# Wiki Article: "
                + title
                + "\n"
                + "## Section Title: "
                + section_title
                + "\n"
                + knowledge_entry.section_texts[it]
            )
        elif (
            "external links" in section_title.lower()
            or "references" in section_title.lower()
        ):
            continue
        else:
            sections.append(
                "# Wiki Article: "
                + title
                + "\n"
                + "## Section Title: "
                + section_title
                + "\n"
                + knowledge_entry.section_texts[it]
            )
    if section_index != -1:
        return evidence_section, sections
    return sections


def get_all_sections(knowledge_entry):
    """Get all sections in list format."""
    sections = []
    for it, section_title in enumerate(knowledge_entry.section_titles):
        sections.append(
            "* Section Title: "
            + section_title
            + "\n"
            + knowledge_entry.section_texts[it]
        )

    return sections


pseudo_tokenizer = None


def _adjust_prompt_length(prompt, desired_token_length):
    """Adjust the prompt length to the desired token length."""
    global pseudo_tokenizer

    if pseudo_tokenizer is None:
        pseudo_tokenizer = AutoTokenizer.from_pretrained(
            "liuhaotian/llava-v1.5-7b"
        )

    # Tokenize the prompt
    tokens = pseudo_tokenizer.encode(prompt)

    if len(tokens) > desired_token_length:
        # If the prompt is too long, trim it
        trimmed_tokens = tokens[:desired_token_length]
        # Convert tokens back to text
        trimmed_text = pseudo_tokenizer.decode(
            trimmed_tokens, clean_up_tokenization_spaces=True
        )[4:]
        return trimmed_text
    else:
        # If the length is already as desired
        return prompt


class AnswerGenerator:
    """Question generator for OMGM."""

    def __init__(self):
        self.model = None

    def load_model(self, model_name):
        """Load the model.

        Args:
            model_name: The model to load.
        """
        raise NotImplementedError

def prompt_constructor(question, dataset_name, entry = None, entry_section = None):

    if entry is not None:
        # setting for article-based VQA (entity level)
        context = reconstruct_wiki_article(entry)
        context = _adjust_prompt_length(context, 3072)
        if dataset_name == "infoseek":
            prompt = (
                "- Context: "
                + context
                + "\n- Question: "
                + question
                + "\nJust answer the questions , no explanations needed. Short answer is:"
            )
        else:
            prompt = (
                "- Context: "
                + context
                + "\n- Question: "
                + question
                + "\nThe answer is:"
            ) #e-vqa setting
    elif entry_section is not None:
        #setting for section-based VQA
        entry_section = _adjust_prompt_length(entry_section, 3072)
        if dataset_name == "infoseek":
            prompt = (
                "- Context: "
                + entry_section
                + "\n- Question: "
                + question
                + "\nJust answer the questions , no explanations needed. Short answer is:"
            )
        else:
            prompt = (
                "- Context: "
                + entry_section
                + "\n- Question: "
                + question
                + "\nThe answer is:"
            ) #e-vqa setting
    else:
        #setting for vanilla VQA
        prompt = "- Question: " + question + "\nThe answer is:"

    return prompt



def Get_dataset_based_messages(dataset_name, prompt):
    
    if dataset_name == 'infoseek':
        messages = [
                {
                    "role": "system",
                    "content": "You are a helpful assistant for answering encyclopedic questions. Do not answer anything else."
                        +"If you need to answer questions about numbers or time, please output the corresponding numerical format directly."
                        +"If the context does not contain the information required to answer the question, you should answer the question using internal model knowledge."
                    },
                {"role": "user", "content":  "- Context: # Wiki Article: Dolomites\n## Section Title: Dolomites\nThe Dolomites, also known as the Dolomite Mountains, Dolomite Alps or Dolomitic Alps, are a mountain range located in northeastern Italy. The Dolomites are located in the regions of Veneto, Trentino-Alto Adige/Südtirol and Friuli Venezia Giulia, covering an area shared between the provinces of Belluno, Vicenza, Verona, Trentino, South Tyrol, Udine and Pordenone."
                        +"\n- Question: Which city or region does this mountain locate in?"
                        +"\nJust answer the questions , no explanations needed. Short answer is: Province of Belluno\n\n"
                        +prompt}
        ]        
    else:
        messages = [
                {
                    "role": "system",
                    "content": "You are a helpful assistant for answering encyclopedic questions."
                    +"If the context does not contain the information required to answer the question, you should answer the question using internal model knowledge."
                },
                {"role": "user", "content": prompt},
        ]
    return messages
        
 
def Get_VLM_dataset_based_messages(dataset_name, prompt):

    
    if dataset_name == "infoseek":
        messages = VLM_INFOSEEK_INSTRUCTION + prompt
    else:
        messages = VLM_EVQA_INSTRUCTION + prompt
    return messages

class InterVL2_5AnswerGenerator(AnswerGenerator):
    def __init__(self, device, model_path):
        """Initialize the QuestionGenerator class.

        Args:
            device: The device to use for the model.
            model_path: The model to load.
        """
        super().__init__()
        self.device = device
        self.model_path = model_path
        self._load_model()

    def _load_model(self):
        """Load the model.

        Args:
            model_path: The model to load.
        """
        disable_torch_init()
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True, use_fast=False)
        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
        self.model = AutoModel.from_pretrained(
            self.model_path,
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True,
            use_flash_attn=True,
            trust_remote_code=True).eval()
        self.model.to(self.device)

    @torch.no_grad()
    def llm_answering(
        self,
        vanilla_input,
        dataset_name,
        entry=None,
        entry_section=None
    ):
        """InterVL2_5 Answering for a given entry

        Args:
            vanilla_input: vanilla input to the model.
            dataset_name: The image dataset name.
            entry: The entry to answer the question for.
            entry_section: The entry section to answer the question for.
            
        """
        question = vanilla_input["question"]
        image_path = vanilla_input["image_path"]
        prompt = prompt_constructor(question, dataset_name, entry, entry_section)
            
        messages = Get_VLM_dataset_based_messages(dataset_name, prompt)

        messages = "<image>\n" + messages
        
        pixel_values = load_image(image_path, max_num=12).to(torch.bfloat16).to(self.device)
        response = self.model.chat(self.tokenizer, pixel_values, messages, dict(max_new_tokens=128, do_sample=False))
        return response

class LlaVA1_5AnswerGenerator(AnswerGenerator):
    def __init__(self, device, model_path):
        """Initialize the QuestionGenerator class.

        Args:
            device: The device to use for the model.
            model_path: The model to load.
        """
        super().__init__()
        self.device = device
        self.model_path = model_path
        if self.model_path == 'llava-hf/llava-1.5-7b-hf':
            self._load_model()
        else:
            from llava.model.builder import load_pretrained_model
            from llava.mm_utils import (
                get_model_name_from_path,
            )
            model_name = get_model_name_from_path(self.model_path)
            self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(
                self.model_path, None, model_name
            )
        self.model.to(self.device)


    def _load_model(self):
        """Load the model.

        Args:
            model_path: The model to load.
        """
        disable_torch_init()
        self.processor = AutoProcessor.from_pretrained(self.model_path)
        self.model = LlavaForConditionalGeneration.from_pretrained(
            self.model_path,
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True,
            device_map='auto',
            # attn_implementation='flash_attention_2',
            ).eval()
        # self.model.to(self.device)
 
    @torch.no_grad()
    def llm_answering(self, vanilla_input, dataset_name, entry=None, entry_section=None):
        """LlaVA1_5 Answering for a given entry
        Args:
            vanilla_input: vanilla input to the model.
            dataset_name: The image dataset name.
            entry: The entry to answer the question for.
            entry_section: The entry section to answer the question for.
        """
        question = vanilla_input["question"]
        image_path = vanilla_input["image_path"]
        raw_image = Image.open(image_path).convert("RGB")

        prompt = prompt_constructor(question, dataset_name, entry, entry_section)

        messages = Get_VLM_dataset_based_messages(dataset_name, prompt)
        if self.model_path == 'llava-hf/llava-1.5-7b-hf':
            conversation = [{
                "role": "user",
                "content": [{"type": "text", "text": messages},{"type": "image"},],
                },]
            input_format = self.processor.apply_chat_template(conversation, add_generation_prompt=True)
            inputs =self.processor(images=raw_image, text=input_format, return_tensors='pt').to(self.device, torch.float16)
            output = self.model.generate(**inputs, max_new_tokens=200, do_sample=False)
            response = self.processor.decode(output[0], skip_special_tokens=True).split("ASSISTANT:")[-1].strip()
            return response
        else:
            from llava.constants import  IMAGE_TOKEN_INDEX
            from llava.mm_utils import process_images, tokenizer_image_token
            # from llava.conversation import conv_template

            image_sizes = [raw_image.size]
            images_tensor = process_images(
                [raw_image],
                self.image_processor,
                self.model.config
            ).to(self.model.device, dtype=torch.float16)
            input_ids = (
                tokenizer_image_token(messages, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors="pt")
                .unsqueeze(0)
                .cuda()
            )
            with torch.inference_mode():
                output_ids = self.model.generate(
                    input_ids,
                    images=images_tensor,
                    image_sizes=image_sizes,
                    do_sample= False,
                    max_new_tokens=200,
                    use_cache=True,
                )
            response = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()
            return response

class Qwen3VLAnswerGenerator(AnswerGenerator):
    """
    Qwen3-VL AnswerGenerator (similar style to your LLaVA1_5AnswerGenerator).
    """

    def __init__(self,
        device: str,
        model_path: str,
        torch_dtype=torch.bfloat16,
        use_device_map_auto: bool = False,):
        super().__init__()
        self.device = device
        self.model_path = model_path
        self.torch_dtype = torch_dtype
        self.use_device_map_auto = use_device_map_auto
        self._load_model()

    def _load_model(self):
        self.processor = AutoProcessor.from_pretrained(self.model_path)
        self.model = Qwen3VLForConditionalGeneration.from_pretrained(
                self.model_path,
                dtype="auto",
                device_map="auto",
            ).eval()

    @torch.no_grad()
    def llm_answering(self, vanilla_input, dataset_name, entry=None, entry_section=None):
        question = vanilla_input["question"]
        image_path = vanilla_input["image_path"]
        # raw_image = Image.open(image_path).convert("RGB")
        prompt = prompt_constructor(question, dataset_name, entry, entry_section)
        messages_text = Get_VLM_dataset_based_messages(dataset_name, prompt)
        messages = [{
                "role": "user",
                "content": [{
                        "type": "image",
                        "image": image_path,
                    },
                    {"type": "text", "text": messages_text},],
            }]
        inputs = self.processor.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt"
        )
        inputs = inputs.to(self.model.device)

        # Inference: Generation of the output
        generated_ids = self.model.generate(**inputs, max_new_tokens=200)
        generated_ids_trimmed = [
            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        response = self.processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )
        return response[0]
    
class BGESectionReranker:
    """BGE Section Reranker"""

    def __init__(self, model_path, device):
        """Initialize the Text Reranker"""
        self.device = device
        self.model_path = model_path
        self._load_model()

    def _load_model(self):
        """Load the model."""
        print("Loading BGE Section Reranker")
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)
        self.model.to(self.device)
        self.model.eval()
        print("BGE Section Reranker Loaded")

    @torch.no_grad()
    def rank_entry_sections(self, question, sections):
        if not sections:
            empty = torch.empty(0, device=self.device, dtype=torch.float32)
            empty_idx = torch.empty(0, device=self.device, dtype=torch.long)
            return empty, empty_idx
        pairs = [[question, section] for section in sections]
        pair_step = 4
        for pair_spilit in range(0, len(pairs), pair_step):
            inputs = self.tokenizer(
                pairs[pair_spilit : pair_spilit + pair_step], padding=True, truncation=True, return_tensors="pt", max_length=6000
            ).to(self.device)
            scores = (
                self.model(**inputs, return_dict=True)
                .logits.view(
                    -1,
                )
                .float()
            )
            if pair_spilit == 0:
                all_scores = scores
            else:
                all_scores = torch.cat([all_scores, scores], dim=0)
        _, index = torch.sort(all_scores, descending=True)

        return torch.sigmoid(all_scores), index
